# Development Plan — v0.2

In-progress milestones for kiso v0.2. Continues from v0.1 (all M1–M43 complete).

## Principles

- **Agile**: smallest testable increment first, then layer on
- **No dead code**: every line written is immediately reachable and testable
- **Fail loud**: missing config, broken provider, invalid input → clear error, never silent fallback
- **Tested**: every milestone adds tests. `uv run pytest` must pass before moving on.

---

## Milestone 44: Deferred items + polish

Collect all items explicitly deferred during v0.1 development, plus small UX fixes observed in production.

### 44a. worker — estrai `search.py` da `loop.py`

**Problema**: M36 prevedeva `kiso/worker/search.py` come modulo dedicato al search handler (analogo a `exec.py` e `skill.py`). La logica è invece rimasta in `loop.py`, rompendo la simmetria del package. Il `__init__.py` non esporta nulla di search-related.

**Fix**: estrarre il search handler da `loop.py` in `kiso/worker/search.py`, analogamente a `exec.py` e `skill.py`. Aggiornare `__init__.py` e gli import.

- [x] `kiso/worker/search.py`: nuovo modulo con `_parse_search_args()` e `_search_task()` estratti da `loop.py`
- [x] `kiso/worker/loop.py`: rimosso il parsing inline degli args e la chiamata `run_searcher` diretta; usa `_search_task()`
- [x] `kiso/worker/__init__.py`: esporta `_parse_search_args` e `_search_task`
- [x] `tests/test_worker.py`: aggiornati i patch target da `kiso.worker.loop.run_searcher` a `kiso.worker.search.run_searcher` (17 occorrenze)
- [x] `tests/test_search.py`: 33 unit test isolati per `_parse_search_args` e `_search_task` (boundary max_results, type errors, malformed JSON, warning log, propagazione parametri)

---

### 44b. CLI — loader on new line

**Problem**: the "thinking..." spinner appears inline with the last content line instead of on its own line. Visually confusing when the previous output ends without a trailing newline.

**Fix**: ensure the loader/spinner always starts on a fresh line.

- [x] `cli/__init__.py`: `_poll_status` gains `_at_col0: bool = True` parameter; emits `\n` before first frame when `_at_col0=False`
- [x] tests: 2 new cases verify `\n\n\r` (extra newline) vs `\n\r` (no extra newline)

---

### 44c. Fact poisoning — `save_learning` content filter

**Context**: during M21 the curator prompt was deemed sufficient to guard against manipulative learnings. Monitor in production whether this holds or whether pre-storage filtering is needed.

- [x] Added `_SENSITIVE_PATTERN` regex + filter in `save_learning()` (keywords: password/passwd/token, hex ≥32 chars); returns 0 + logs warning
- [x] 7 unit tests: keyword rejection (case-insensitive), hex threshold, benign acceptance, sentinel preservation

---

### 44d. Verbose mode — incremental LLM rendering

**Context**: `append_task_llm_call()` was added in M31 but the worker still batches all LLM call data at task end. Documented as known limitation in `docs/cli.md`.

**Fix**: call `append_task_llm_call()` after each individual LLM call (searcher, exec translator, reviewer, messenger) so verbose panels appear incrementally during task execution.

- [x] `kiso/worker/loop.py`: `_append_calls()` helper added; called after each LLM op (translator, reviewer, messenger, searcher) in all task branches and fast-path
- [x] `llm_calls=` removed from all `update_task_usage` call sites; `_KEEP_LLM_CALLS` sentinel preserves incrementally-appended column
- [x] `docs/cli.md`: removed "Known limitations — batch rendering" note
- [x] 5 tests: assert `_append_calls` call count per task type; verify `update_task_usage` called without explicit `llm_calls`

---

### 44e. Post-review hardening — M44b/M44c/M44d follow-up

Problemi di alta/media severità emersi dalla review di M44b+M44d.

#### Alta severità

- [x] `cli/__init__.py`: docstring completa per `_poll_status()` con spiegazione del parametro `_at_col0`
- [x] `kiso/store.py`: `save_learning()` — TypeError su `content=None`; return 0 su stringa vuota/whitespace; docstring aggiornata
- [x] `kiso/worker/loop.py`: `_append_calls()` — `try/except Exception` con log.warning; nessun crash del worker per errori di usage tracking
- [x] `kiso/worker/loop.py`: `_fast_path_chat()` — `_append_calls()` chiamata anche nel failure path (eccez. messenger)

#### Media severità

- [x] `tests/test_store.py`: guard None (TypeError), stringa vuota, whitespace, boundary hex 31 chars (accettato) vs 32 chars (rifiutato)
- [x] `tests/test_worker.py`: `_append_calls` handles exception, fast_path failure appends, success vs failure same call_count; retry scenario verifica 5 `_append_calls` (2 tentativi × 2 + 1 msg)
- [x] `tests/test_cli.py`: `\n\n\r` appare esattamente una volta — extra newline solo al primo frame, non ai frame successivi

---

### 44f. Admin context — session labels on facts

**Context**: M43 implemented strict session scoping for regular users (Option B). Admin users already bypass the filter and receive all facts, but with no indication of which session each fact originated from, making cross-session facts indistinguishable in the planner context.

**Fix**: when building planner context for an admin caller, split facts into two priority tiers:
- **Primary** (`## Known Facts`): facts from the current session + global facts (session IS NULL). Shown without session label — these are the most relevant.
- **Background** (`## Context from Other Sessions`): facts from other sessions, annotated with `[session:<name>]`. Available as broader memory if needed.

Non-admin path is unchanged (single `## Known Facts` block, no labels).

- [x] `kiso/brain.py`: `build_planner_messages()` — admin path splits `facts` into `primary` and `other`; `_group_by_category()` helper renders both blocks; non-admin uses `primary=facts, other=[]`
- [x] `tests/test_brain.py`: `test_admin_facts_hierarchy` — current-session + global facts in `## Known Facts` (no label); other-session fact in `## Context from Other Sessions` with `[session:name]`; current-session fact never carries a label
- [x] `tests/test_brain.py`: `test_non_admin_facts_no_session_labels` — non-admin context has no `[session:` and no `## Context from Other Sessions`

---

## Milestone 45: Planner — plugin discovery via registry (not web search)

**Problema osservato in produzione**: l'utente chiede "voglio installare connettore discord" e il planner emette `[search] kiso discord connector` (web search) invece di `[exec] curl <registry_url>`. Il reviewer correttamente rigetta il risultato ("sa-mp discord connector", non quello kiso), ma il ciclo è sprecato.

**Cause root**:
1. La rule "Prefer search over exec curl/wget for web lookups" nel task type `search` non aveva l'eccezione esplicita per il registry kiso.
2. Le regole MANDATORY sulla discovery via registry erano seppellite in fondo e non collegavano esplicitamente l'eccezione alla rule search.
3. `kiso` non era in `PROBE_BINARIES` → non compariva nella lista "available binaries" del System Environment.

**Fix**:

- [x] `kiso/roles/planner.md`: aggiunta eccezione esplicita nella description del task `search`: "NEVER use search to discover kiso plugins — use exec curl on the registry URL instead (see Plugin installation rule below)"
- [x] `kiso/roles/planner.md`: regole MANDATORY unificate e riscritte come "Plugin installation (MANDATORY)": unico blocco che copre discovery (exec curl registry, mai web search) + install + env requirements
- [x] `kiso/sysenv.py`: aggiunto `"kiso"` in testa a `PROBE_BINARIES` — il binario appare in "available binaries" se installato, rendendo esplicita la sua disponibilità al planner
- [x] `tests/test_brain.py`: `test_m45_plugin_install_uses_registry_not_search` — verifica che il prompt contenga "NEVER" + "registry" + "web search"
- [x] `tests/test_brain.py`: `test_m45_plugin_install_rule_is_mandatory` — verifica che "Plugin installation (MANDATORY)" sia nel prompt
- [x] `tests/test_sysenv.py`: `test_kiso_in_probe_list` — verifica che `"kiso"` sia in `PROBE_BINARIES`
- [x] `docs/flow.md`: aggiornate sezioni "Facts" (era "global", ora "session-scoped"), "Planner Context" (aggiunto esempio admin con `## Context from Other Sessions`)
- [x] `docs/security-risks.md`: aggiornate due sezioni obsolete ("Facts are global" → scoping M43 + residual risk per categorie globali)
- [x] `docs/llm-roles.md`: aggiornata tabella contesto ("Facts (global)" → "Facts (session-scoped; admin sees all)")

---

## Milestone 47: Planner/Reviewer — qualità dei piani e correttezza della valutazione

Problemi osservati in produzione durante l'installazione di Discord. Tre difetti distinti che si sono concatenati causando un loop di replan inutile fino al timeout.

---

### 47a. Planner — autoconsapevolezza e disambiguazione intent

**Problema**: il planner non sa di essere il cervello di un sistema specifico (Kiso). Si comporta come un task planner generico su OS. Questo causa due difetti concatenati: (1) non distingue tra richieste che riguardano il layer OS e quelle che riguardano il layer Kiso; (2) ha un bias verso l'esecuzione anche quando l'intent è ambiguo.

**Causa root**: il prompt inizia con "You are a task planner." — nessuna identità, nessun contesto su Kiso, nessuna consapevolezza dei due layer disponibili.

**Soluzione concordata — due interventi sul prompt del planner**:

**(1) Autoconsapevolezza** — aggiungere in testa al prompt:
- Identità: il planner è il cervello di Kiso, non un planner generico
- Two-layer awareness: ha a disposizione sia comandi OS sia primitive Kiso native (skill, connector, env, memoria); prima di pianificare una soluzione OS, verificare se esiste già una soluzione Kiso-native. Scoped alla scelta della soluzione, non come preferenza globale assoluta.

**(2) Bias di disambiguazione** — invertire il default sulla clarifica:
- Attuale (permissivo): "if unclear, ask" — il planner deve giustificare il chiedere
- Nuovo (restrittivo): procedi solo se intent E target sono entrambi non ambigui; altrimenti chiedi
- Una riga sola, nessun caso enumerato; la two-layer awareness è il contesto che rende applicabile la regola

- [x] `kiso/roles/planner.md`: aggiunta identity Kiso + two-layer awareness in testa al prompt
- [x] `kiso/roles/planner.md`: regola clarifica invertita — "proceed only if unambiguous, else ask"
- [x] `kiso/roles/planner.md`: guida su `expect` task-scoped
- [x] `docs/llm-roles.md`: sezione "Identity and Environment Awareness" aggiunta al Planner
- [x] `docs/llm-roles.md`: campo `expect` aggiornato con nota su scope task
- [x] `tests/test_brain.py`: `TestM47PlannerIdentityAndTwoLayer` — 5 test su identity, two-layer, Kiso-native preference, clarification bias, expect scoping

---

### 47b. Reviewer — valuta `expect` del task, non il goal del piano

**Problema**: il reviewer usa il Plan Goal come criterio di successo invece dell'`expect` del singolo task. Caso concreto: `apt-get install -f` con output "0 upgraded, 0 installed" (= nessuna dipendenza rotta = successo) è stato marcato `replan` perché "Discord non è ancora fully functional" — che è il goal del piano, non il criterio del task.

**Causa root**: il prompt del reviewer riceve il `Plan Goal` come contesto ma non distingue esplicitamente tra "context" e "success criterion". Il modello tende a unificarli.

**Soluzione concordata — due interventi**:

1. **Rinominare** il campo `Plan Goal` → `Plan Context` nel messaggio costruito da `build_reviewer_messages` in `brain.py`. Il nome cambia il frame a livello strutturale: il modello vede un dato coerente con la regola invece di un dato che la contraddice.

2. **Aggiungere una regola** al prompt del reviewer: "Plan Context is background only. The sole success criterion is the task's `expect`." Output "nothing to do" / "0 changes" per comandi di manutenzione è successo valido se coerente con l'`expect`.

- [x] `kiso/brain.py`: `build_reviewer_messages` — rinominato `## Plan Goal` → `## Plan Context`
- [x] `kiso/roles/reviewer.md`: aggiunta regola "Plan Goal is background context only" + regola su "0 changes"
- [x] `docs/llm-roles.md`: tabella contesto aggiornata ("Plan context (goal) — as background"); `goal` output field aggiornato
- [x] `tests/test_brain.py`: `TestM47ReviewerPlanContext` — 4 test su label, regola background, sole criterion, zero-changes

---

### 47c. Planner — `expect` scoped al piano, non al task

**Problema**: il planner scrive `expect` che descrivono il goal globale del piano invece dell'output atteso del singolo task. Esempio: per `apt-get install -f`, il planner ha scritto `"All dependencies resolved, Discord should be fully functional"` — che è il goal del piano. Questo causa il problema 47b.

**Causa root**: nessuna istruzione nel prompt del planner su come scrivere `expect` task-scoped. Il planner propaga il goal del piano nell'`expect` di ogni task, invece di descrivere cosa deve produrre/mostrare quel task specifico.

**Nota**: 47c è indipendente da 47b e necessario anche dopo il fix del reviewer. Un reviewer correttamente calibrato che ignora il Plan Context valuta comunque l'`expect` letteralmente — se l'`expect` dice "Discord should be fully functional", il reviewer è costretto a marcare `apt-get install -f` come fallito anche se ha ragione. Il problema è a monte: il planner ha scritto l'`expect` sbagliato.

**Soluzione concordata**: aggiungere al prompt del planner una guida esplicita su come scrivere `expect`:
- descrive l'output diretto di questo specifico task, non il goal del piano
- deve essere falsificabile dall'output del task da solo
- per comandi di manutenzione/cleanup (apt-get install -f, git clean, ecc.), "0 changes" è un successo valido e va detto esplicitamente nell'expect

- [x] implementato insieme a 47a (`kiso/roles/planner.md` + test `test_planner_prompt_expect_scoping`)

---

### 47d. Worker (exec translator) — retry hint ignorato

**Problema**: al retry, il worker ignora l'hint nel Retry Context e ri-traduce letteralmente il `detail` del task originale, producendo lo stesso comando fallito. Nel caso Discord, l'hint diceva "usa `apt install discord`" ma il worker ha riprodotto `apt-get install -f` perché il `detail` del task lo specificava.

**Causa root**: il prompt del worker non menziona il Retry Context. Il worker non sa che in caso di retry l'hint dovrebbe guidare la traduzione alternativa.

**Soluzione concordata**: aggiungere al prompt del worker che in presenza di Retry Context con hint, l'hint ha priorità sulla traduzione letterale del `detail`. Il `detail` rimane contesto per capire il task, ma è l'hint che guida il comando da produrre.

- [x] `kiso/roles/worker.md`: aggiunta regola hint priority
- [x] `docs/llm-roles.md`: regola aggiunta in "Rules in the Default Prompt" per l'exec translator
- [x] `tests/test_brain.py`: `TestM47WorkerHintPriority` — test su presenza regola nel prompt

---

## Milestone 48: Prompt hygiene — ottimizzazioni trasversali ai role prompts

Ottimizzazioni identificate da revisione sistematica di tutti i role prompt. Nessun nuovo comportamento: solo coerenza, riduzione ridondanza, e colmare gap di istruzioni che il modello attualmente risolve per inferenza (con risultati variabili).

---

### 48a. Reviewer — "You receive" incoerente con il messaggio reale

**Problema**: il prompt descrive "You receive: The plan goal" ma da M47 il messaggio arriva etichettato `## Plan Context`. Il modello vede un'etichetta diversa da quella che il prompt descrive — incoerenza strutturale.

**Fix**: allineare la descrizione nel prompt a `Plan Context`.

- [x] `kiso/roles/reviewer.md`: "You receive: The plan goal" → "The plan context"
- [x] `tests/test_brain.py`: `TestM48ReviewerPromptHygiene` — test_48a_you_receive_says_plan_context, test_48a_no_plan_goal_in_receive_block

---

### 48b. Reviewer — exit code non usato nelle regole

**Problema**: il reviewer riceve `## Command Status` (succeeded/FAILED exit code) ma nessuna regola dice come usarlo. Il modello lo interpreta autonomamente. In casi ambigui (output apparentemente ok ma exit code 1, o stderr con exit code 0) può sbagliare direzione senza istruzioni esplicite.

**Fix**: aggiungere regola esplicita: exit code non-zero è un forte segnale di fallimento anche se l'output appare parzialmente ok; exit code 0 non è sufficiente da solo se l'output non soddisfa l'`expect`.

- [x] `kiso/roles/reviewer.md`: aggiunta regola "Exit code is a strong signal: non-zero = forte indicatore di fallimento; zero = necessario ma non sufficiente"
- [x] `tests/test_brain.py`: `TestM48ReviewerPromptHygiene` — test_48b_exit_code_rule_present, test_48b_nonzero_is_failure_signal, test_48b_zero_not_sufficient_alone

---

### 48c. Planner — regole `expect` e `detail` frammentate

**Problema**: due coppie di regole concettualmente identiche sono spezzate e distanziate nel prompt:
1. Riga 21 (`expect non-null`) + riga 28 (`expect task-scoped`) — stessa regola in due pezzi
2. Riga 26 (`detail self-contained`) + riga 46 (`detail specific, include commands/paths`) — stessa istruzione ripetuta; la seconda (più importante) arriva ultima e viene letta con meno attenzione

**Fix**: unire ciascuna coppia in una regola sola, spostata in posizione rilevante. Riduce il testo totale del prompt.

- [x] `kiso/roles/planner.md`: regola expect unificata (non-null + task-scoped in una riga); regola detail unificata (self-contained + specific + commands/paths in una riga); rimossa riga ridondante "exec task detail must be specific"
- [x] `tests/test_brain.py`: `TestM48PlannerMergedRules` — 6 test su presenza regole unificate e assenza duplicati

---

### 48d. Curator — nessuna categoria nel promote

**Problema**: il curator promuove fatti come testo libero senza indicare la categoria (project/user/tool/general). L'informazione viene persa e il consolidator deve inferirla. Se il curator distinguesse già il tipo di fatto, la categorizzazione successiva sarebbe più affidabile.

**Fix**: aggiungere campo `category` al JSON di output del curator. `_apply_curator_result` usa la categoria per determinare il session scoping: solo `"user"` è session-scoped, gli altri sono globali. Bug fix: i fatti non-user non devono più essere salvati con `session=session`.

- [x] `kiso/roles/curator.md`: aggiunta istruzione category per promote (project/user/tool/general)
- [x] `kiso/brain.py`: `CURATOR_SCHEMA` — aggiunto campo `category` nullable con enum; aggiunto a `required`
- [x] `kiso/brain.py`: `validate_curator` — validazione opzionale: se category != null, deve essere un valore valido
- [x] `kiso/worker/loop.py`: `_apply_curator_result` — usa `ev.get("category") or "general"`; session=session solo se category=="user", altrimenti session=None
- [x] `tests/test_brain.py`: `TestM48CuratorCategoryField` — 11 test su prompt, schema, validate_curator
- [x] `tests/test_worker.py`: `TestM48ApplyCuratorCategory` — 9 test su session scoping per ogni categoria

---

### 48e. Summarizer-facts — nessun tiebreaker per contraddizioni

**Problema**: il prompt dice "remove contradictory items" ma non specifica quale tenere quando due fatti si contraddicono. Il modello sceglie arbitrariamente.

**Fix**: aggiungere criterio esplicito di risoluzione: in caso di contraddizione, preferire il fatto con confidence più alta; se uguale, preferire il più specifico.

- [x] `kiso/roles/summarizer-facts.md`: aggiunta regola tiebreaker (higher confidence wins; if equal, more specific wins)
- [x] `tests/test_brain.py`: `TestM48SummarizerFactsTiebreaker` — 3 test su presenza regola tiebreaker

---

### 48f. Worker (exec translator) — nessuna regola su sudo

**Problema**: nessuna istruzione su `sudo`. In ambienti non-root, il planner può scrivere task detail che implicano privilegi elevati, e il worker aggiunge `sudo` autonomamente o non lo aggiunge — comportamento non deterministico.

**Fix**: aggiungere regola: non aggiungere `sudo` a meno che non sia esplicitamente menzionato nel task detail o nel system environment.

- [x] `kiso/roles/worker.md`: aggiunta regola "Do NOT add sudo unless explicitly mentioned"
- [x] `tests/test_brain.py`: `TestM48WorkerNoSudo` — 3 test su presenza regola no-sudo
- [x] `docs/llm-roles.md`: aggiornate sezioni Reviewer (exit code rule), Exec Translator (no-sudo), Curator (schema con category, verdicts), Summarizer-facts (tiebreaker)

---

## Milestone 49: Versioning — file versione + comando `kiso version`

**Problema**: la versione del progetto era implicita (solo in `pyproject.toml`). Non c'era un file canonico da usare come riferimento quando si scrive la versione in docs/devplan, né un comando CLI per stamparla.

**Fix**: file unico `kiso/_version.py` come source of truth + comando `kiso version` + flag `--version/-V`.

- [x] `kiso/_version.py`: `__version__ = "0.1.0"` — unica fonte di verità per la versione
- [x] `cli/__init__.py`: importa `__version__`; aggiunge `-V/--version` flag (argparse built-in); aggiunge `version` subcommand che stampa `kiso {__version__}`; descrizione parser include la versione
- [x] `tests/test_cli.py`: `TestVersionFile` (3 test: file exists, semver format, cli re-export) e `TestVersionCommand` (5 test: subcommand, print output, -V, --version, help includes version)

---

## Milestone 50: Multi-instance — named bots via Docker

**Contesto**: `kiso-host.sh` e `install.sh` esistono già e funzionano. L'architettura Docker-native è il deployment model corrente (single instance, container `kiso`, porta 8333 hardcoded, dati in `~/.kiso/`). M50 estende il wrapper per supportare più istanze named sulla stessa macchina senza toccare il core Python.

**Principio**: ogni istanza è un bot con un nome univoco. Il core non sa nulla di "istanze" — è un'immagine Docker che vede solo `/root/.kiso`. Tutta la gestione è nel wrapper host.

---

### Layout locale

```
~/.kiso/
  instances.json                   # registro (vedi sotto)
  instances/
    jarvis/                        # montato come /root/.kiso in kiso-jarvis
      config.toml
      .env
      kiso.db
      sessions/
      roles/
      connectors/
        discord/
          config.toml              # webhook_port = 9001 (porta del range di jarvis)
    work/
      config.toml                  # config separata (modelli, provider, utenti diversi)
      connectors/
        discord/
          config.toml              # webhook_port = 9101 (porta del range di work)
```

**`~/.kiso/instances.json`**:
```json
{
  "jarvis": { "server_port": 8333, "connector_port_base": 9000 },
  "work":   { "server_port": 8334, "connector_port_base": 9100 }
}
```

---

### Porte — strategia

**Server port**: auto-detect primo libero partendo da 8333.

**Connector port base**: auto-detect primo multiplo di 100 libero partendo da 9000 (range di 10 porte per istanza: base+1...base+10). Docker espone l'intero range: `-p {base+1}-{base+10}:{base+1}-{base+10}`. Porta interna = porta esterna (no NAT asimmetrico — il connector conosce la propria porta esterna senza configurazione extra).

Quando `kiso connector install NAME` viene eseguito tramite wrapper:
1. Il wrapper esegue l'install nel container (come ora)
2. Assegna la prossima porta libera dal range dell'istanza
3. Sovrascrive `webhook_port` nel `config.toml` del connector appena installato
4. Registra la porta in `instances.json`: `connectors: { discord: 9001 }`

---

### Risoluzione istanza implicita

Applicata a tutti i comandi se `--instance` non è specificato:
- 0 istanze → errore: `"Nessun bot configurato. Esegui: kiso instance create NAME"`
- 1 istanza → usata implicitamente (zero overhead per l'utente single-bot)
- 2+ istanze → errore: `"Più bot disponibili. Specifica: kiso --instance NAME"`

---

### Comandi — nuovo wrapper

**Gestione istanze** (tutti nel wrapper bash, non nel CLI Python):

```bash
kiso instance create NAME          # crea dir, docker run, registra in instances.json
kiso instance start [NAME]         # docker start kiso-{NAME}
kiso instance stop [NAME]          # docker stop kiso-{NAME}
kiso instance restart [NAME]       # docker restart kiso-{NAME}
kiso instance list                 # nome / server port / connector base / status
kiso instance status [NAME]        # stato container + health check
kiso instance logs [NAME] [-f]     # docker logs kiso-{NAME}
kiso instance shell [NAME]         # docker exec -it kiso-{NAME} bash
kiso instance explore [NAME] [session]   # shell nel workspace della sessione
kiso instance remove [NAME] [--yes]      # docker rm + rm -rf instances/{NAME}/
```

**Comandi esistenti** — invariati, ricevono `--instance` opzionale:
```bash
kiso [--instance NAME]                   # REPL
kiso [--instance NAME] msg "..."
kiso [--instance NAME] env set KEY VAL
kiso [--instance NAME] sessions
kiso [--instance NAME] skill install X
kiso [--instance NAME] connector run discord
kiso [--instance NAME] reset session
```

**Comandi che spariscono** (assorbiti in `kiso instance *`):
`kiso up`, `kiso down`, `kiso restart`, `kiso status`, `kiso health`, `kiso logs`, `kiso shell`, `kiso explore`

---

### Validazione nome istanza

Applicata ovunque l'utente fornisce un nome (install.sh + `kiso instance create`):

- Formato: `^[a-z0-9][a-z0-9-]*$` (o `^[a-z0-9]$` per nomi 1 char), max 32 caratteri
- Motivazione: compatibile con DNS hostname syntax, Docker container names, path di directory
- Errore chiaro se non valido: `"Instance name must be lowercase alphanumeric + hyphens, no leading/trailing hyphen"`

---

### `install.sh` — flusso aggiornato

1. Check prerequisiti (docker, git) — invariato
2. **Chiede il nome del bot** (default: `kiso`) — validato con regex sopra
3. Chiede username, bot_name, provider, API key, modelli — invariato
4. Crea `~/.kiso/instances/NAME/config.toml` e `.env` (invece di `~/.kiso/config.toml`)
5. Auto-detect porta server e connector base
6. Build immagine Docker — invariato
7. `docker run -d --name kiso-{NAME} -p {server}:8333 -p {cbase+1}-{cbase+10}:{cbase+1}-{cbase+10} -v ~/.kiso/instances/{NAME}:/root/.kiso kiso:latest`
8. Registra in `~/.kiso/instances.json`
9. Installa wrapper e completions — invariato

Re-install (`./install.sh` su installazione esistente) offre: aggiungere nuova istanza o riconfigurare esistente.

---

### Dockerfile — CMD → uvicorn diretto

```dockerfile
# Prima (rimosso):
CMD ["uv", "run", "kiso", "serve"]

# Dopo:
CMD ["uv", "run", "uvicorn", "kiso.server:app", "--host", "0.0.0.0", "--port", "8333"]
```

Elimina la dipendenza da `kiso serve` a runtime. Permette di rimuovere `kiso serve` dal parser Python senza rompere nulla.

---

### Modifiche al core Python

- `cli/__init__.py`: rimozione di `kiso serve` dal parser e da `main()` (il server non si avvia più via CLI)
- Nient'altro cambia nel core

---

### File da aggiornare

| File | Tipo di modifica |
|---|---|
| `kiso-host.sh` | riscrittura completa |
| `install.sh` | refactor flusso (bot name + multi-instance) |
| `Dockerfile` | CMD → uvicorn |
| `docker-compose.yml` | rimosso o ridotto a tool di sviluppo |
| `completions/kiso.bash` + `kiso.zsh` | aggiunta `instance *` |
| `cli/__init__.py` | rimozione `kiso serve` |
| `docs/docker.md` | riscrittura completa |
| `docs/cli.md` | aggiornamento command reference |
| `docs/config.md` | path `~/.kiso/instances/{name}/` |
| `docs/connectors.md` | path + connector port strategy |
| `docs/logging.md` | path aggiornati |

---

### Subtask

- [x] 50a. Core Python: rimozione `kiso serve` dal parser + Dockerfile CMD → uvicorn
- [x] 50b. `kiso-host.sh`: riscrittura — multi-instance, port resolution, `kiso instance *`, risoluzione implicita, connector port assignment su install
- [x] 50c. `install.sh`: bot name + `~/.kiso/instances/{name}/` + `instances.json` + docker run con port range
- [x] 50d. Docs: `docker.md` (riscrittura), `cli.md`, `config.md`, `connectors.md`, `logging.md` + `completions/`

---

## Milestone 46: Plugin install — ask env vars before installing, with how-to instructions

**Problema osservato in produzione**: il planner installava il connettore discord, poi scopriva le env var mancanti dall'output di install, e solo alla fine chiedeva i valori all'utente. Doppio problema:
1. L'utente aspetta tutto il processo di install prima di essere interrogato.
2. Il messaggio all'utente non spiegava come ottenere i valori (dove trovare il bot token, cosa è il kiso token, ecc.).

**Cause root**:
1. Il prompt del planner diceva "install, then check" — ordine sbagliato.
2. `kiso.toml` del connettore discord non aveva `description` nei campi env var.
3. `cli/connector.py` stampava solo `warning: VAR not set`, senza descrizione.

**Fix**:

- [x] `plugins/connector-discord/kiso.toml`: aggiunto campo `description` a ogni env var (`bot_token`, `kiso_token`, `webhook_secret`) con istruzioni su come ottenerli
- [x] `cli/connector.py`: `_connector_install()` — stampa `(required|optional) — {description}` accanto a ogni warning di env var mancante; usa `decl.items()` invece di `for key in env_decl` per accedere alle descrizioni
- [x] `kiso/roles/planner.md`: regola "Plugin installation (MANDATORY)" riscritta come lista ordinata di 6 step: (1) curl registry, (2) curl kiso.toml da GitHub PRIMA di installare, (3) msg user con descrizioni se env var mancanti, (4) kiso env set, (5) install, (6) run
- [x] `tests/test_cli_connector.py`: `test_connector_install_env_warning_includes_description` — verifica che l'output di install contenga la description accanto al warning per required e optional
- [x] `tests/test_brain.py`: `test_m46_plugin_install_checks_kiso_toml_before_install` — verifica che raw.githubusercontent.com/kiso.toml appaia prima di `kiso connector install` nel prompt
- [x] `tests/test_brain.py`: `test_m46_plugin_install_includes_env_description_in_msg` — verifica che il prompt istruisca il planner a includere le descrizioni nel messaggio all'utente
- [x] `kiso/roles/messenger.md`: aggiunta regola "verbatim" — le istruzioni di setup (comandi, URL, step-by-step) devono essere riportate esatte, non parafrasate
- [x] `tests/test_brain.py`: `test_verbatim_instructions_rule` — verifica che "verbatim" sia nel prompt del messenger

---

## Milestone 51: Infrastruttura — test automatici kiso-host.sh e install.sh

**Problema**: `kiso-host.sh` e `install.sh` hanno zero copertura automatica. La logica di validazione nomi, risoluzione istanza implicita, port detection e comandi `kiso instance *` è completamente manual-tested.

**Framework**: [bats-core](https://github.com/bats-core/bats-core) — standard de facto per bash testing. Installato come dev dependency (`tests/bash/bats/` via git submodule o download in CI). Nessuna dipendenza runtime.

**Strategia di mock**: `PATH` viene anteposto con una directory `tests/bash/mocks/` contenente stub per `docker`, `git`, `ss`, `python3` (instances.json), ecc. I test verificano flag, argomenti e side-effect osservabili (exit code, output, file scritti).

### 51a. Test `kiso-host.sh`

- [ ] `tests/bash/test_host_name_validation.bats`: `validate_name()` — validi (`kiso`, `my-bot`, `a`), invalidi (`MyBot`, `-bot`, `bot_name`, stringa di 33 char, stringa vuota)
- [ ] `tests/bash/test_host_resolve_instance.bats`: 0 istanze → exit 1 + messaggio; 1 istanza → implicita; 2+ istanze senza `--instance` → exit 1; 2+ istanze con `--instance NAME` → NAME usato
- [ ] `tests/bash/test_host_instance_commands.bats`: `instance list`, `instance start`, `instance stop`, `instance restart`, `instance status`, `instance logs`, `instance shell` — verifica che il container giusto venga targetato e che i flag docker siano corretti
- [ ] `tests/bash/test_host_instance_remove.bats`: senza `--yes` → chiede conferma; con `--yes` → rimuove senza prompt; container inesistente → exit 1 chiaro
- [ ] `tests/bash/test_host_instance_explore.bats`: SESSION inesistente nella directory → exit 1 con messaggio; SESSION esistente → `docker exec` con `-e SESSION=...`
- [ ] `tests/bash/test_host_port_detection.bats`: `_next_free_server_port()` salta porte già in uso (mock `ss`); `_next_free_connector_base()` salta base già in `instances.json`

### 51b. Test `install.sh`

- [ ] `tests/bash/test_install_name_validation.bats`: stesse regole di 51a — validi, invalidi, duplicato già in `instances.json`
- [ ] `tests/bash/test_install_port_allocation.bats`: `next_free_server_port()` e `next_free_connector_base()` con mock di `instances.json` con istanze esistenti
- [ ] `tests/bash/test_install_register.bats`: `register_instance()` crea `instances.json` se mancante; aggiunge entry senza sovrascrivere istanze esistenti

---

## Milestone 52: Core — fix istanze multi-instance

Tre fix puntuali a `kiso-host.sh` emersi dall'analisi post-M50.

### 52a. `kiso instance explore` — validazione SESSION

**Problema**: `kiso instance explore SESSION` entra nel container anche se `SESSION` non esiste come directory workspace, producendo una shell in `/` senza contesto utile.

**Fix**: prima di `docker exec`, verificare che `~/.kiso/instances/{name}/sessions/{session}/` esista sul host. Se mancante → exit 1 con messaggio "session '{session}' not found in instance '{name}'". Se `SESSION` omesso → shell in `/root/.kiso/` (comportamento attuale, utile per debug).

- [x] `kiso-host.sh`: controllo `[ -d "$KISO_DIR/instances/$INST/sessions/$SESSION" ]` prima di `docker exec`; SESSION=default (hostname@user) quando omesso
- [x] `tests/bash/test_host_instance_explore.bats`: aggiornato — mkdir sessione in test "with SESSION"; nuovo test "SESSION not found → error"

### 52b. `kiso instance remove` — conferma interattiva se `--yes` assente

**Problema**: senza `--yes`, il comando esce silenziosamente (o fa il remove senza prompt — da verificare). Nessun test lo copre oggi.

**Fix**: se `--yes` assente → prompt interattivo "Remove instance '{name}' and all its data? [y/N]"; default N; solo `y`/`Y` procede.

- [x] `kiso-host.sh`: prompt interattivo nel branch `remove` quando `--yes` non presente (già implementato in M50)
- [x] `tests/bash/test_host_instance_remove.bats`: coperto in M51 (5 test: --yes, -y, 'y' stdin, 'n' abort, keeps others)

### 52c. `kiso instance create` — porta già in uso al momento del run

**Problema**: `_next_free_server_port()` controlla porte al momento della detect; ma tra detect e `docker run` un'altra istanza potrebbe averla occupata. Oggi silenzioso: docker run fallisce con errore poco chiaro.

**Fix**: dopo `docker run`, verificare che il container sia in stato `running` (retry 1s × 3). Se non parte → `docker logs kiso-{NAME} | tail -20` + messaggio "container failed to start — see above" + cleanup entry da `instances.json`.

- [x] `kiso-host.sh`: health check con retry (3×) dopo docker run; skip instances.json write su failure; `docker rm -f` cleanup
- [x] `tests/bash/test_host_instance_commands.bats`: test `create` fallisce → instances.json non contiene l'entry; `sleep` mock in helpers per test istantanei

---

## Milestone 53: Versioning — line count nel comando `kiso version`

**Obiettivo**: `kiso version --stats` mostra il conteggio righe di codice del progetto, suddiviso per area, calcolato in Python (no `wc -l` — per portabilità e controllo su cosa contare).

**Cosa si conta**: righe non-vuote e non-commento (lines of code, LOC) nei file `.py` del progetto. Tre aree:

| Area | Path | Descrizione |
|---|---|---|
| `core` | `kiso/` | Framework principale |
| `cli` | `cli/` | CLI interattiva |
| `tests` | `tests/` | Suite di test |

**Output**:
```
kiso 0.1.0

  core     3 241 loc   (kiso/)
  cli      1 088 loc   (cli/)
  tests    8 470 loc   (tests/)
  ─────────────────
  total   12 799 loc
```

**Implementazione**:

- [x] `kiso/_version.py`: nuova funzione `count_loc(root: Path) -> dict` + helper `_loc_in_dir(directory)` — rglob *.py, conta righe non-vuote e non-commento
- [x] `cli/__init__.py`: `version_p.add_argument("--stats")`; `_print_version_stats()` chiama `count_loc`, formatta output con separatore migliaia (spazio); senza `--stats` → comportamento attuale
- [x] `tests/test_version.py`: `TestLocCounter` — 5 test passati

---
