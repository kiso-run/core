# Development Plan — v0.2

In-progress milestones for kiso v0.2. Continues from v0.1 (all M1–M43 complete).

## Principles

- **Agile**: smallest testable increment first, then layer on
- **No dead code**: every line written is immediately reachable and testable
- **Fail loud**: missing config, broken provider, invalid input → clear error, never silent fallback
- **Tested**: every milestone adds tests. `uv run pytest` must pass before moving on.

---

## Milestone 44: Deferred items + polish

Collect all items explicitly deferred during v0.1 development, plus small UX fixes observed in production.

### 44a. worker — estrai `search.py` da `loop.py`

**Problema**: M36 prevedeva `kiso/worker/search.py` come modulo dedicato al search handler (analogo a `exec.py` e `skill.py`). La logica è invece rimasta in `loop.py`, rompendo la simmetria del package. Il `__init__.py` non esporta nulla di search-related.

**Fix**: estrarre il search handler da `loop.py` in `kiso/worker/search.py`, analogamente a `exec.py` e `skill.py`. Aggiornare `__init__.py` e gli import.

- [x] `kiso/worker/search.py`: nuovo modulo con `_parse_search_args()` e `_search_task()` estratti da `loop.py`
- [x] `kiso/worker/loop.py`: rimosso il parsing inline degli args e la chiamata `run_searcher` diretta; usa `_search_task()`
- [x] `kiso/worker/__init__.py`: esporta `_parse_search_args` e `_search_task`
- [x] `tests/test_worker.py`: aggiornati i patch target da `kiso.worker.loop.run_searcher` a `kiso.worker.search.run_searcher` (17 occorrenze)
- [x] `tests/test_search.py`: 33 unit test isolati per `_parse_search_args` e `_search_task` (boundary max_results, type errors, malformed JSON, warning log, propagazione parametri)

---

### 44b. CLI — loader on new line

**Problem**: the "thinking..." spinner appears inline with the last content line instead of on its own line. Visually confusing when the previous output ends without a trailing newline.

**Fix**: ensure the loader/spinner always starts on a fresh line.

- [x] `cli/__init__.py`: `_poll_status` gains `_at_col0: bool = True` parameter; emits `\n` before first frame when `_at_col0=False`
- [x] tests: 2 new cases verify `\n\n\r` (extra newline) vs `\n\r` (no extra newline)

---

### 44c. Fact poisoning — `save_learning` content filter

**Context**: during M21 the curator prompt was deemed sufficient to guard against manipulative learnings. Monitor in production whether this holds or whether pre-storage filtering is needed.

- [x] Added `_SENSITIVE_PATTERN` regex + filter in `save_learning()` (keywords: password/passwd/token, hex ≥32 chars); returns 0 + logs warning
- [x] 7 unit tests: keyword rejection (case-insensitive), hex threshold, benign acceptance, sentinel preservation

---

### 44d. Verbose mode — incremental LLM rendering

**Context**: `append_task_llm_call()` was added in M31 but the worker still batches all LLM call data at task end. Documented as known limitation in `docs/cli.md`.

**Fix**: call `append_task_llm_call()` after each individual LLM call (searcher, exec translator, reviewer, messenger) so verbose panels appear incrementally during task execution.

- [x] `kiso/worker/loop.py`: `_append_calls()` helper added; called after each LLM op (translator, reviewer, messenger, searcher) in all task branches and fast-path
- [x] `llm_calls=` removed from all `update_task_usage` call sites; `_KEEP_LLM_CALLS` sentinel preserves incrementally-appended column
- [x] `docs/cli.md`: removed "Known limitations — batch rendering" note
- [x] 5 tests: assert `_append_calls` call count per task type; verify `update_task_usage` called without explicit `llm_calls`

---

### 44e. Post-review hardening — M44b/M44c/M44d follow-up

Problemi di alta/media severità emersi dalla review di M44b+M44d.

#### Alta severità

- [x] `cli/__init__.py`: docstring completa per `_poll_status()` con spiegazione del parametro `_at_col0`
- [x] `kiso/store.py`: `save_learning()` — TypeError su `content=None`; return 0 su stringa vuota/whitespace; docstring aggiornata
- [x] `kiso/worker/loop.py`: `_append_calls()` — `try/except Exception` con log.warning; nessun crash del worker per errori di usage tracking
- [x] `kiso/worker/loop.py`: `_fast_path_chat()` — `_append_calls()` chiamata anche nel failure path (eccez. messenger)

#### Media severità

- [x] `tests/test_store.py`: guard None (TypeError), stringa vuota, whitespace, boundary hex 31 chars (accettato) vs 32 chars (rifiutato)
- [x] `tests/test_worker.py`: `_append_calls` handles exception, fast_path failure appends, success vs failure same call_count; retry scenario verifica 5 `_append_calls` (2 tentativi × 2 + 1 msg)
- [x] `tests/test_cli.py`: `\n\n\r` appare esattamente una volta — extra newline solo al primo frame, non ai frame successivi

---

### 44f. Admin context — session labels on facts

**Context**: M43 implemented strict session scoping for regular users (Option B). Admin users already bypass the filter and receive all facts, but with no indication of which session each fact originated from, making cross-session facts indistinguishable in the planner context.

**Fix**: when building planner context for an admin caller, split facts into two priority tiers:
- **Primary** (`## Known Facts`): facts from the current session + global facts (session IS NULL). Shown without session label — these are the most relevant.
- **Background** (`## Context from Other Sessions`): facts from other sessions, annotated with `[session:<name>]`. Available as broader memory if needed.

Non-admin path is unchanged (single `## Known Facts` block, no labels).

- [x] `kiso/brain.py`: `build_planner_messages()` — admin path splits `facts` into `primary` and `other`; `_group_by_category()` helper renders both blocks; non-admin uses `primary=facts, other=[]`
- [x] `tests/test_brain.py`: `test_admin_facts_hierarchy` — current-session + global facts in `## Known Facts` (no label); other-session fact in `## Context from Other Sessions` with `[session:name]`; current-session fact never carries a label
- [x] `tests/test_brain.py`: `test_non_admin_facts_no_session_labels` — non-admin context has no `[session:` and no `## Context from Other Sessions`

---

## Milestone 45: Planner — plugin discovery via registry (not web search)

**Problema osservato in produzione**: l'utente chiede "voglio installare connettore discord" e il planner emette `[search] kiso discord connector` (web search) invece di `[exec] curl <registry_url>`. Il reviewer correttamente rigetta il risultato ("sa-mp discord connector", non quello kiso), ma il ciclo è sprecato.

**Cause root**:
1. La rule "Prefer search over exec curl/wget for web lookups" nel task type `search` non aveva l'eccezione esplicita per il registry kiso.
2. Le regole MANDATORY sulla discovery via registry erano seppellite in fondo e non collegavano esplicitamente l'eccezione alla rule search.
3. `kiso` non era in `PROBE_BINARIES` → non compariva nella lista "available binaries" del System Environment.

**Fix**:

- [x] `kiso/roles/planner.md`: aggiunta eccezione esplicita nella description del task `search`: "NEVER use search to discover kiso plugins — use exec curl on the registry URL instead (see Plugin installation rule below)"
- [x] `kiso/roles/planner.md`: regole MANDATORY unificate e riscritte come "Plugin installation (MANDATORY)": unico blocco che copre discovery (exec curl registry, mai web search) + install + env requirements
- [x] `kiso/sysenv.py`: aggiunto `"kiso"` in testa a `PROBE_BINARIES` — il binario appare in "available binaries" se installato, rendendo esplicita la sua disponibilità al planner
- [x] `tests/test_brain.py`: `test_m45_plugin_install_uses_registry_not_search` — verifica che il prompt contenga "NEVER" + "registry" + "web search"
- [x] `tests/test_brain.py`: `test_m45_plugin_install_rule_is_mandatory` — verifica che "Plugin installation (MANDATORY)" sia nel prompt
- [x] `tests/test_sysenv.py`: `test_kiso_in_probe_list` — verifica che `"kiso"` sia in `PROBE_BINARIES`
- [x] `docs/flow.md`: aggiornate sezioni "Facts" (era "global", ora "session-scoped"), "Planner Context" (aggiunto esempio admin con `## Context from Other Sessions`)
- [x] `docs/security-risks.md`: aggiornate due sezioni obsolete ("Facts are global" → scoping M43 + residual risk per categorie globali)
- [x] `docs/llm-roles.md`: aggiornata tabella contesto ("Facts (global)" → "Facts (session-scoped; admin sees all)")

---

## Milestone 47: Planner/Reviewer — qualità dei piani e correttezza della valutazione

Problemi osservati in produzione durante l'installazione di Discord. Tre difetti distinti che si sono concatenati causando un loop di replan inutile fino al timeout.

---

### 47a. Planner — autoconsapevolezza e disambiguazione intent

**Problema**: il planner non sa di essere il cervello di un sistema specifico (Kiso). Si comporta come un task planner generico su OS. Questo causa due difetti concatenati: (1) non distingue tra richieste che riguardano il layer OS e quelle che riguardano il layer Kiso; (2) ha un bias verso l'esecuzione anche quando l'intent è ambiguo.

**Causa root**: il prompt inizia con "You are a task planner." — nessuna identità, nessun contesto su Kiso, nessuna consapevolezza dei due layer disponibili.

**Soluzione concordata — due interventi sul prompt del planner**:

**(1) Autoconsapevolezza** — aggiungere in testa al prompt:
- Identità: il planner è il cervello di Kiso, non un planner generico
- Two-layer awareness: ha a disposizione sia comandi OS sia primitive Kiso native (skill, connector, env, memoria); prima di pianificare una soluzione OS, verificare se esiste già una soluzione Kiso-native. Scoped alla scelta della soluzione, non come preferenza globale assoluta.

**(2) Bias di disambiguazione** — invertire il default sulla clarifica:
- Attuale (permissivo): "if unclear, ask" — il planner deve giustificare il chiedere
- Nuovo (restrittivo): procedi solo se intent E target sono entrambi non ambigui; altrimenti chiedi
- Una riga sola, nessun caso enumerato; la two-layer awareness è il contesto che rende applicabile la regola

- [x] `kiso/roles/planner.md`: aggiunta identity Kiso + two-layer awareness in testa al prompt
- [x] `kiso/roles/planner.md`: regola clarifica invertita — "proceed only if unambiguous, else ask"
- [x] `kiso/roles/planner.md`: guida su `expect` task-scoped
- [x] `docs/llm-roles.md`: sezione "Identity and Environment Awareness" aggiunta al Planner
- [x] `docs/llm-roles.md`: campo `expect` aggiornato con nota su scope task
- [x] `tests/test_brain.py`: `TestM47PlannerIdentityAndTwoLayer` — 5 test su identity, two-layer, Kiso-native preference, clarification bias, expect scoping

---

### 47b. Reviewer — valuta `expect` del task, non il goal del piano

**Problema**: il reviewer usa il Plan Goal come criterio di successo invece dell'`expect` del singolo task. Caso concreto: `apt-get install -f` con output "0 upgraded, 0 installed" (= nessuna dipendenza rotta = successo) è stato marcato `replan` perché "Discord non è ancora fully functional" — che è il goal del piano, non il criterio del task.

**Causa root**: il prompt del reviewer riceve il `Plan Goal` come contesto ma non distingue esplicitamente tra "context" e "success criterion". Il modello tende a unificarli.

**Soluzione concordata — due interventi**:

1. **Rinominare** il campo `Plan Goal` → `Plan Context` nel messaggio costruito da `build_reviewer_messages` in `brain.py`. Il nome cambia il frame a livello strutturale: il modello vede un dato coerente con la regola invece di un dato che la contraddice.

2. **Aggiungere una regola** al prompt del reviewer: "Plan Context is background only. The sole success criterion is the task's `expect`." Output "nothing to do" / "0 changes" per comandi di manutenzione è successo valido se coerente con l'`expect`.

- [x] `kiso/brain.py`: `build_reviewer_messages` — rinominato `## Plan Goal` → `## Plan Context`
- [x] `kiso/roles/reviewer.md`: aggiunta regola "Plan Goal is background context only" + regola su "0 changes"
- [x] `docs/llm-roles.md`: tabella contesto aggiornata ("Plan context (goal) — as background"); `goal` output field aggiornato
- [x] `tests/test_brain.py`: `TestM47ReviewerPlanContext` — 4 test su label, regola background, sole criterion, zero-changes

---

### 47c. Planner — `expect` scoped al piano, non al task

**Problema**: il planner scrive `expect` che descrivono il goal globale del piano invece dell'output atteso del singolo task. Esempio: per `apt-get install -f`, il planner ha scritto `"All dependencies resolved, Discord should be fully functional"` — che è il goal del piano. Questo causa il problema 47b.

**Causa root**: nessuna istruzione nel prompt del planner su come scrivere `expect` task-scoped. Il planner propaga il goal del piano nell'`expect` di ogni task, invece di descrivere cosa deve produrre/mostrare quel task specifico.

**Nota**: 47c è indipendente da 47b e necessario anche dopo il fix del reviewer. Un reviewer correttamente calibrato che ignora il Plan Context valuta comunque l'`expect` letteralmente — se l'`expect` dice "Discord should be fully functional", il reviewer è costretto a marcare `apt-get install -f` come fallito anche se ha ragione. Il problema è a monte: il planner ha scritto l'`expect` sbagliato.

**Soluzione concordata**: aggiungere al prompt del planner una guida esplicita su come scrivere `expect`:
- descrive l'output diretto di questo specifico task, non il goal del piano
- deve essere falsificabile dall'output del task da solo
- per comandi di manutenzione/cleanup (apt-get install -f, git clean, ecc.), "0 changes" è un successo valido e va detto esplicitamente nell'expect

- [x] implementato insieme a 47a (`kiso/roles/planner.md` + test `test_planner_prompt_expect_scoping`)

---

### 47d. Worker (exec translator) — retry hint ignorato

**Problema**: al retry, il worker ignora l'hint nel Retry Context e ri-traduce letteralmente il `detail` del task originale, producendo lo stesso comando fallito. Nel caso Discord, l'hint diceva "usa `apt install discord`" ma il worker ha riprodotto `apt-get install -f` perché il `detail` del task lo specificava.

**Causa root**: il prompt del worker non menziona il Retry Context. Il worker non sa che in caso di retry l'hint dovrebbe guidare la traduzione alternativa.

**Soluzione concordata**: aggiungere al prompt del worker che in presenza di Retry Context con hint, l'hint ha priorità sulla traduzione letterale del `detail`. Il `detail` rimane contesto per capire il task, ma è l'hint che guida il comando da produrre.

- [x] `kiso/roles/worker.md`: aggiunta regola hint priority
- [x] `docs/llm-roles.md`: regola aggiunta in "Rules in the Default Prompt" per l'exec translator
- [x] `tests/test_brain.py`: `TestM47WorkerHintPriority` — test su presenza regola nel prompt

---

## Milestone 48: Prompt hygiene — ottimizzazioni trasversali ai role prompts

Ottimizzazioni identificate da revisione sistematica di tutti i role prompt. Nessun nuovo comportamento: solo coerenza, riduzione ridondanza, e colmare gap di istruzioni che il modello attualmente risolve per inferenza (con risultati variabili).

---

### 48a. Reviewer — "You receive" incoerente con il messaggio reale

**Problema**: il prompt descrive "You receive: The plan goal" ma da M47 il messaggio arriva etichettato `## Plan Context`. Il modello vede un'etichetta diversa da quella che il prompt descrive — incoerenza strutturale.

**Fix**: allineare la descrizione nel prompt a `Plan Context`.

- [x] `kiso/roles/reviewer.md`: "You receive: The plan goal" → "The plan context"
- [x] `tests/test_brain.py`: `TestM48ReviewerPromptHygiene` — test_48a_you_receive_says_plan_context, test_48a_no_plan_goal_in_receive_block

---

### 48b. Reviewer — exit code non usato nelle regole

**Problema**: il reviewer riceve `## Command Status` (succeeded/FAILED exit code) ma nessuna regola dice come usarlo. Il modello lo interpreta autonomamente. In casi ambigui (output apparentemente ok ma exit code 1, o stderr con exit code 0) può sbagliare direzione senza istruzioni esplicite.

**Fix**: aggiungere regola esplicita: exit code non-zero è un forte segnale di fallimento anche se l'output appare parzialmente ok; exit code 0 non è sufficiente da solo se l'output non soddisfa l'`expect`.

- [x] `kiso/roles/reviewer.md`: aggiunta regola "Exit code is a strong signal: non-zero = forte indicatore di fallimento; zero = necessario ma non sufficiente"
- [x] `tests/test_brain.py`: `TestM48ReviewerPromptHygiene` — test_48b_exit_code_rule_present, test_48b_nonzero_is_failure_signal, test_48b_zero_not_sufficient_alone

---

### 48c. Planner — regole `expect` e `detail` frammentate

**Problema**: due coppie di regole concettualmente identiche sono spezzate e distanziate nel prompt:
1. Riga 21 (`expect non-null`) + riga 28 (`expect task-scoped`) — stessa regola in due pezzi
2. Riga 26 (`detail self-contained`) + riga 46 (`detail specific, include commands/paths`) — stessa istruzione ripetuta; la seconda (più importante) arriva ultima e viene letta con meno attenzione

**Fix**: unire ciascuna coppia in una regola sola, spostata in posizione rilevante. Riduce il testo totale del prompt.

- [x] `kiso/roles/planner.md`: regola expect unificata (non-null + task-scoped in una riga); regola detail unificata (self-contained + specific + commands/paths in una riga); rimossa riga ridondante "exec task detail must be specific"
- [x] `tests/test_brain.py`: `TestM48PlannerMergedRules` — 6 test su presenza regole unificate e assenza duplicati

---

### 48d. Curator — nessuna categoria nel promote

**Problema**: il curator promuove fatti come testo libero senza indicare la categoria (project/user/tool/general). L'informazione viene persa e il consolidator deve inferirla. Se il curator distinguesse già il tipo di fatto, la categorizzazione successiva sarebbe più affidabile.

**Fix**: aggiungere campo `category` al JSON di output del curator. `_apply_curator_result` usa la categoria per determinare il session scoping: solo `"user"` è session-scoped, gli altri sono globali. Bug fix: i fatti non-user non devono più essere salvati con `session=session`.

- [x] `kiso/roles/curator.md`: aggiunta istruzione category per promote (project/user/tool/general)
- [x] `kiso/brain.py`: `CURATOR_SCHEMA` — aggiunto campo `category` nullable con enum; aggiunto a `required`
- [x] `kiso/brain.py`: `validate_curator` — validazione opzionale: se category != null, deve essere un valore valido
- [x] `kiso/worker/loop.py`: `_apply_curator_result` — usa `ev.get("category") or "general"`; session=session solo se category=="user", altrimenti session=None
- [x] `tests/test_brain.py`: `TestM48CuratorCategoryField` — 11 test su prompt, schema, validate_curator
- [x] `tests/test_worker.py`: `TestM48ApplyCuratorCategory` — 9 test su session scoping per ogni categoria

---

### 48e. Summarizer-facts — nessun tiebreaker per contraddizioni

**Problema**: il prompt dice "remove contradictory items" ma non specifica quale tenere quando due fatti si contraddicono. Il modello sceglie arbitrariamente.

**Fix**: aggiungere criterio esplicito di risoluzione: in caso di contraddizione, preferire il fatto con confidence più alta; se uguale, preferire il più specifico.

- [x] `kiso/roles/summarizer-facts.md`: aggiunta regola tiebreaker (higher confidence wins; if equal, more specific wins)
- [x] `tests/test_brain.py`: `TestM48SummarizerFactsTiebreaker` — 3 test su presenza regola tiebreaker

---

### 48f. Worker (exec translator) — nessuna regola su sudo

**Problema**: nessuna istruzione su `sudo`. In ambienti non-root, il planner può scrivere task detail che implicano privilegi elevati, e il worker aggiunge `sudo` autonomamente o non lo aggiunge — comportamento non deterministico.

**Fix**: aggiungere regola: non aggiungere `sudo` a meno che non sia esplicitamente menzionato nel task detail o nel system environment.

- [x] `kiso/roles/worker.md`: aggiunta regola "Do NOT add sudo unless explicitly mentioned"
- [x] `tests/test_brain.py`: `TestM48WorkerNoSudo` — 3 test su presenza regola no-sudo
- [x] `docs/llm-roles.md`: aggiornate sezioni Reviewer (exit code rule), Exec Translator (no-sudo), Curator (schema con category, verdicts), Summarizer-facts (tiebreaker)

---

## Milestone 49: Versioning — file versione + comando `kiso version`

**Problema**: la versione del progetto era implicita (solo in `pyproject.toml`). Non c'era un file canonico da usare come riferimento quando si scrive la versione in docs/devplan, né un comando CLI per stamparla.

**Fix**: file unico `kiso/_version.py` come source of truth + comando `kiso version` + flag `--version/-V`.

- [x] `kiso/_version.py`: `__version__ = "0.1.0"` — unica fonte di verità per la versione
- [x] `cli/__init__.py`: importa `__version__`; aggiunge `-V/--version` flag (argparse built-in); aggiunge `version` subcommand che stampa `kiso {__version__}`; descrizione parser include la versione
- [x] `tests/test_cli.py`: `TestVersionFile` (3 test: file exists, semver format, cli re-export) e `TestVersionCommand` (5 test: subcommand, print output, -V, --version, help includes version)

---

## Milestone 50: Multi-instance — named bots via Docker

**Contesto**: `kiso-host.sh` e `install.sh` esistono già e funzionano. L'architettura Docker-native è il deployment model corrente (single instance, container `kiso`, porta 8333 hardcoded, dati in `~/.kiso/`). M50 estende il wrapper per supportare più istanze named sulla stessa macchina senza toccare il core Python.

**Principio**: ogni istanza è un bot con un nome univoco. Il core non sa nulla di "istanze" — è un'immagine Docker che vede solo `/root/.kiso`. Tutta la gestione è nel wrapper host.

---

### Layout locale

```
~/.kiso/
  instances.json                   # registro (vedi sotto)
  instances/
    jarvis/                        # montato come /root/.kiso in kiso-jarvis
      config.toml
      .env
      kiso.db
      sessions/
      roles/
      connectors/
        discord/
          config.toml              # webhook_port = 9001 (porta del range di jarvis)
    work/
      config.toml                  # config separata (modelli, provider, utenti diversi)
      connectors/
        discord/
          config.toml              # webhook_port = 9101 (porta del range di work)
```

**`~/.kiso/instances.json`**:
```json
{
  "jarvis": { "server_port": 8333, "connector_port_base": 9000 },
  "work":   { "server_port": 8334, "connector_port_base": 9100 }
}
```

---

### Porte — strategia

**Server port**: auto-detect primo libero partendo da 8333.

**Connector port base**: auto-detect primo multiplo di 100 libero partendo da 9000 (range di 10 porte per istanza: base+1...base+10). Docker espone l'intero range: `-p {base+1}-{base+10}:{base+1}-{base+10}`. Porta interna = porta esterna (no NAT asimmetrico — il connector conosce la propria porta esterna senza configurazione extra).

Quando `kiso connector install NAME` viene eseguito tramite wrapper:
1. Il wrapper esegue l'install nel container (come ora)
2. Assegna la prossima porta libera dal range dell'istanza
3. Sovrascrive `webhook_port` nel `config.toml` del connector appena installato
4. Registra la porta in `instances.json`: `connectors: { discord: 9001 }`

---

### Risoluzione istanza implicita

Applicata a tutti i comandi se `--instance` non è specificato:
- 0 istanze → errore: `"Nessun bot configurato. Esegui: kiso instance create NAME"`
- 1 istanza → usata implicitamente (zero overhead per l'utente single-bot)
- 2+ istanze → errore: `"Più bot disponibili. Specifica: kiso --instance NAME"`

---

### Comandi — nuovo wrapper

**Gestione istanze** (tutti nel wrapper bash, non nel CLI Python):

```bash
kiso instance create NAME          # crea dir, docker run, registra in instances.json
kiso instance start [NAME]         # docker start kiso-{NAME}
kiso instance stop [NAME]          # docker stop kiso-{NAME}
kiso instance restart [NAME]       # docker restart kiso-{NAME}
kiso instance list                 # nome / server port / connector base / status
kiso instance status [NAME]        # stato container + health check
kiso instance logs [NAME] [-f]     # docker logs kiso-{NAME}
kiso instance shell [NAME]         # docker exec -it kiso-{NAME} bash
kiso instance explore [NAME] [session]   # shell nel workspace della sessione
kiso instance remove [NAME] [--yes]      # docker rm + rm -rf instances/{NAME}/
```

**Comandi esistenti** — invariati, ricevono `--instance` opzionale:
```bash
kiso [--instance NAME]                   # REPL
kiso [--instance NAME] msg "..."
kiso [--instance NAME] env set KEY VAL
kiso [--instance NAME] sessions
kiso [--instance NAME] skill install X
kiso [--instance NAME] connector run discord
kiso [--instance NAME] reset session
```

**Comandi che spariscono** (assorbiti in `kiso instance *`):
`kiso up`, `kiso down`, `kiso restart`, `kiso status`, `kiso health`, `kiso logs`, `kiso shell`, `kiso explore`

---

### Validazione nome istanza

Applicata ovunque l'utente fornisce un nome (install.sh + `kiso instance create`):

- Formato: `^[a-z0-9][a-z0-9-]*$` (o `^[a-z0-9]$` per nomi 1 char), max 32 caratteri
- Motivazione: compatibile con DNS hostname syntax, Docker container names, path di directory
- Errore chiaro se non valido: `"Instance name must be lowercase alphanumeric + hyphens, no leading/trailing hyphen"`

---

### `install.sh` — flusso aggiornato

1. Check prerequisiti (docker, git) — invariato
2. **Chiede il nome del bot** (default: `kiso`) — validato con regex sopra
3. Chiede username, bot_name, provider, API key, modelli — invariato
4. Crea `~/.kiso/instances/NAME/config.toml` e `.env` (invece di `~/.kiso/config.toml`)
5. Auto-detect porta server e connector base
6. Build immagine Docker — invariato
7. `docker run -d --name kiso-{NAME} -p {server}:8333 -p {cbase+1}-{cbase+10}:{cbase+1}-{cbase+10} -v ~/.kiso/instances/{NAME}:/root/.kiso kiso:latest`
8. Registra in `~/.kiso/instances.json`
9. Installa wrapper e completions — invariato

Re-install (`./install.sh` su installazione esistente) offre: aggiungere nuova istanza o riconfigurare esistente.

---

### Dockerfile — CMD → uvicorn diretto

```dockerfile
# Prima (rimosso):
CMD ["uv", "run", "kiso", "serve"]

# Dopo:
CMD ["uv", "run", "uvicorn", "kiso.server:app", "--host", "0.0.0.0", "--port", "8333"]
```

Elimina la dipendenza da `kiso serve` a runtime. Permette di rimuovere `kiso serve` dal parser Python senza rompere nulla.

---

### Modifiche al core Python

- `cli/__init__.py`: rimozione di `kiso serve` dal parser e da `main()` (il server non si avvia più via CLI)
- Nient'altro cambia nel core

---

### File da aggiornare

| File | Tipo di modifica |
|---|---|
| `kiso-host.sh` | riscrittura completa |
| `install.sh` | refactor flusso (bot name + multi-instance) |
| `Dockerfile` | CMD → uvicorn |
| `docker-compose.yml` | rimosso o ridotto a tool di sviluppo |
| `completions/kiso.bash` + `kiso.zsh` | aggiunta `instance *` |
| `cli/__init__.py` | rimozione `kiso serve` |
| `docs/docker.md` | riscrittura completa |
| `docs/cli.md` | aggiornamento command reference |
| `docs/config.md` | path `~/.kiso/instances/{name}/` |
| `docs/connectors.md` | path + connector port strategy |
| `docs/logging.md` | path aggiornati |

---

### Subtask

- [x] 50a. Core Python: rimozione `kiso serve` dal parser + Dockerfile CMD → uvicorn
- [x] 50b. `kiso-host.sh`: riscrittura — multi-instance, port resolution, `kiso instance *`, risoluzione implicita, connector port assignment su install
- [x] 50c. `install.sh`: bot name + `~/.kiso/instances/{name}/` + `instances.json` + docker run con port range
- [x] 50d. Docs: `docker.md` (riscrittura), `cli.md`, `config.md`, `connectors.md`, `logging.md` + `completions/`

---

## Milestone 46: Plugin install — ask env vars before installing, with how-to instructions

**Problema osservato in produzione**: il planner installava il connettore discord, poi scopriva le env var mancanti dall'output di install, e solo alla fine chiedeva i valori all'utente. Doppio problema:
1. L'utente aspetta tutto il processo di install prima di essere interrogato.
2. Il messaggio all'utente non spiegava come ottenere i valori (dove trovare il bot token, cosa è il kiso token, ecc.).

**Cause root**:
1. Il prompt del planner diceva "install, then check" — ordine sbagliato.
2. `kiso.toml` del connettore discord non aveva `description` nei campi env var.
3. `cli/connector.py` stampava solo `warning: VAR not set`, senza descrizione.

**Fix**:

- [x] `plugins/connector-discord/kiso.toml`: aggiunto campo `description` a ogni env var (`bot_token`, `kiso_token`, `webhook_secret`) con istruzioni su come ottenerli
- [x] `cli/connector.py`: `_connector_install()` — stampa `(required|optional) — {description}` accanto a ogni warning di env var mancante; usa `decl.items()` invece di `for key in env_decl` per accedere alle descrizioni
- [x] `kiso/roles/planner.md`: regola "Plugin installation (MANDATORY)" riscritta come lista ordinata di 6 step: (1) curl registry, (2) curl kiso.toml da GitHub PRIMA di installare, (3) msg user con descrizioni se env var mancanti, (4) kiso env set, (5) install, (6) run
- [x] `tests/test_cli_connector.py`: `test_connector_install_env_warning_includes_description` — verifica che l'output di install contenga la description accanto al warning per required e optional
- [x] `tests/test_brain.py`: `test_m46_plugin_install_checks_kiso_toml_before_install` — verifica che raw.githubusercontent.com/kiso.toml appaia prima di `kiso connector install` nel prompt
- [x] `tests/test_brain.py`: `test_m46_plugin_install_includes_env_description_in_msg` — verifica che il prompt istruisca il planner a includere le descrizioni nel messaggio all'utente
- [x] `kiso/roles/messenger.md`: aggiunta regola "verbatim" — le istruzioni di setup (comandi, URL, step-by-step) devono essere riportate esatte, non parafrasate
- [x] `tests/test_brain.py`: `test_verbatim_instructions_rule` — verifica che "verbatim" sia nel prompt del messenger

---

## Milestone 51: Infrastruttura — test automatici kiso-host.sh e install.sh

**Problema**: `kiso-host.sh` e `install.sh` hanno zero copertura automatica. La logica di validazione nomi, risoluzione istanza implicita, port detection e comandi `kiso instance *` è completamente manual-tested.

**Framework**: [bats-core](https://github.com/bats-core/bats-core) — standard de facto per bash testing. Installato come dev dependency (`tests/bash/bats/` via git submodule o download in CI). Nessuna dipendenza runtime.

**Strategia di mock**: `PATH` viene anteposto con una directory `tests/bash/mocks/` contenente stub per `docker`, `git`, `ss`, `python3` (instances.json), ecc. I test verificano flag, argomenti e side-effect osservabili (exit code, output, file scritti).

### 51a. Test `kiso-host.sh`

- [x] `tests/bash/test_host_name_validation.bats`: `validate_name()` — validi (`kiso`, `my-bot`, `a`), invalidi (`MyBot`, `-bot`, `bot_name`, stringa di 33 char, stringa vuota)
- [x] `tests/bash/test_host_resolve_instance.bats`: 0 istanze → exit 1 + messaggio; 1 istanza → implicita; 2+ istanze senza `--instance` → exit 1; 2+ istanze con `--instance NAME` → NAME usato
- [x] `tests/bash/test_host_instance_commands.bats`: `instance list`, `instance start`, `instance stop`, `instance restart`, `instance logs`, `instance shell` — verifica che il container giusto venga targetato e che i flag docker siano corretti
- [x] `tests/bash/test_host_instance_remove.bats`: senza `--yes` → chiede conferma; con `--yes` → rimuove senza prompt; 'n' → abort
- [x] `tests/bash/test_host_instance_explore.bats`: SESSION inesistente nella directory → exit 1 con messaggio; SESSION esistente → `docker exec` con `-e SESSION=...`
- [x] `tests/bash/test_host_port_detection.bats`: `_next_free_server_port()` salta porte già in uso (mock `ss`); `_next_free_connector_base()` salta base già in `instances.json`

### 51b. Test `install.sh`

- [x] `tests/bash/test_install_name_validation.bats`: stesse regole di 51a — validi, invalidi, duplicato già in `instances.json`
- [x] `tests/bash/test_install_port_allocation.bats`: `next_free_server_port()` e `next_free_connector_base()` con mock di `instances.json` con istanze esistenti
- [x] `tests/bash/test_install_register.bats`: `register_instance()` crea `instances.json` se mancante; aggiunge entry senza sovrascrivere istanze esistenti

---

## Milestone 52: Core — fix istanze multi-instance

Tre fix puntuali a `kiso-host.sh` emersi dall'analisi post-M50.

### 52a. `kiso instance explore` — validazione SESSION

**Problema**: `kiso instance explore SESSION` entra nel container anche se `SESSION` non esiste come directory workspace, producendo una shell in `/` senza contesto utile.

**Fix**: prima di `docker exec`, verificare che `~/.kiso/instances/{name}/sessions/{session}/` esista sul host. Se mancante → exit 1 con messaggio "session '{session}' not found in instance '{name}'". Se `SESSION` omesso → shell in `/root/.kiso/` (comportamento attuale, utile per debug).

- [x] `kiso-host.sh`: controllo `[ -d "$KISO_DIR/instances/$INST/sessions/$SESSION" ]` prima di `docker exec`; SESSION=default (hostname@user) quando omesso
- [x] `tests/bash/test_host_instance_explore.bats`: aggiornato — mkdir sessione in test "with SESSION"; nuovo test "SESSION not found → error"

### 52b. `kiso instance remove` — conferma interattiva se `--yes` assente

**Problema**: senza `--yes`, il comando esce silenziosamente (o fa il remove senza prompt — da verificare). Nessun test lo copre oggi.

**Fix**: se `--yes` assente → prompt interattivo "Remove instance '{name}' and all its data? [y/N]"; default N; solo `y`/`Y` procede.

- [x] `kiso-host.sh`: prompt interattivo nel branch `remove` quando `--yes` non presente (già implementato in M50)
- [x] `tests/bash/test_host_instance_remove.bats`: coperto in M51 (5 test: --yes, -y, 'y' stdin, 'n' abort, keeps others)

### 52c. `kiso instance create` — porta già in uso al momento del run

**Problema**: `_next_free_server_port()` controlla porte al momento della detect; ma tra detect e `docker run` un'altra istanza potrebbe averla occupata. Oggi silenzioso: docker run fallisce con errore poco chiaro.

**Fix**: dopo `docker run`, verificare che il container sia in stato `running` (retry 1s × 3). Se non parte → `docker logs kiso-{NAME} | tail -20` + messaggio "container failed to start — see above" + cleanup entry da `instances.json`.

- [x] `kiso-host.sh`: health check con retry (3×) dopo docker run; skip instances.json write su failure; `docker rm -f` cleanup
- [x] `tests/bash/test_host_instance_commands.bats`: test `create` fallisce → instances.json non contiene l'entry; `sleep` mock in helpers per test istantanei

---

## Milestone 53: Versioning — line count nel comando `kiso version`

**Obiettivo**: `kiso version --stats` mostra il conteggio righe di codice del progetto, suddiviso per area, calcolato in Python (no `wc -l` — per portabilità e controllo su cosa contare).

**Cosa si conta**: righe non-vuote e non-commento (lines of code, LOC) nei file `.py` del progetto. Tre aree:

| Area | Path | Descrizione |
|---|---|---|
| `core` | `kiso/` | Framework principale |
| `cli` | `cli/` | CLI interattiva |
| `tests` | `tests/` | Suite di test |

**Output**:
```
kiso 0.1.0

  core     3 241 loc   (kiso/)
  cli      1 088 loc   (cli/)
  tests    8 470 loc   (tests/)
  ─────────────────
  total   12 799 loc
```

**Implementazione**:

- [x] `kiso/_version.py`: nuova funzione `count_loc(root: Path) -> dict` + helper `_loc_in_dir(directory)` — rglob *.py, conta righe non-vuote e non-commento
- [x] `cli/__init__.py`: `version_p.add_argument("--stats")`; `_print_version_stats()` chiama `count_loc`, formatta output con separatore migliaia (spazio); senza `--stats` → comportamento attuale
- [x] `tests/test_version.py`: `TestLocCounter` — 5 test passati

---

## Milestone 54: `kiso stats` — token usage per sessione e istanza

**Obiettivo**: comando `kiso stats` che aggrega i dati di consumo token (già presenti nell'audit log) per modello, sessione o ruolo. Il wrapper aggiunge differenziazione per istanza con `--all`.

### Stato attuale

L'infrastruttura di raccolta dati è già completa:
- `kiso/audit.py`: ogni chiamata LLM genera una riga JSONL `{type, session, role, model, provider, input_tokens, output_tokens, duration_ms, status, timestamp}` in `~/.kiso/instances/{name}/audit/YYYY-MM-DD.jsonl`
- `kiso/llm.py`: token tracciati in-memory via contextvars e scritti nell'audit
- `kiso/store.py`: token accumulati per task (`tasks.input_tokens/output_tokens`) e per piano (`plans.total_input_tokens/output_tokens`)

Manca solo il livello di aggregazione e presentazione.

### 54a. Engine di aggregazione (`kiso/stats.py`)

- [x] `kiso/stats.py`: `read_audit_entries`, `aggregate`, `estimate_cost`, `MODEL_PRICES`
- [x] `tests/test_stats.py`: 16 unit test (6 read_audit_entries, 6 aggregate, 4 estimate_cost)

### 54b. API endpoint `GET /admin/stats`

- [x] `kiso/main.py`: `GET /admin/stats` — admin-only, params `since`, `session`, `by`
- [x] `tests/test_stats.py`: 4 endpoint test (admin guard, empty rows, aggregate by model, session filter)

### 54c. CLI `kiso stats`

- [x] `cli/stats.py`: `run_stats_command`, `print_stats`, `_fmt_k`, `_fmt_cost`
- [x] `cli/__init__.py`: subparser `stats` + dispatch + `/stats` REPL slash command + `_slash_stats`
- [x] `tests/test_cli.py`: 3 test (subcommand exists, flags registered, defaults correct)
- [x] `docs/cli.md`: sezione "Token Usage Statistics" con esempi e output formattato

### 54d. Wrapper `kiso stats [--all]` in `kiso-host.sh`

- [x] `kiso-host.sh`: branch `stats)` — risolve istanza, delega via `docker exec`; `--all` loopa tutte le istanze con header
- [x] `tests/bash/test_host_stats.bats`: 4 test (docker exec chiamato, --since passato, --all headers, instance not running)

---

## Milestone 55: Autocomplete — shell, REPL, wrapper

**Obiettivo**: completamento TAB coerente su tutti e tre i livelli — shell bash/zsh (comandi e argomenti dinamici), REPL interattivo (slash commands), CLI Python (subparser registrati).

### 55a. CLI `kiso completion bash|zsh` + handler wrapper

- [x] `kiso/completions/kiso.bash` e `kiso/completions/kiso.zsh`: script bundled nel package Python (via importlib.resources)
- [x] `cli/__init__.py`: subparser `completion shell` + dispatch che legge da `kiso/completions/`
- [x] `kiso-host.sh completion`: delega a `docker exec kiso-$INST uv run kiso completion {shell}`; fallback a sistema installato
- [x] `tests/test_cli.py`: 4 test (bash contiene `_kiso`/`complete -F`; zsh contiene `#compdef kiso`; stats in entrambi)
- [x] `tests/bash/test_host_completion.bats`: 3 test (bash output, zsh output, shell non valido → errore)

### 55b. Completions aggiornate — `stats` + multi-instance sessions + `explore`

- [x] `kiso/completions/kiso.bash`: aggiunto `stats` nei comandi top-level; branch `stats)` con `--since/--session/--by/--all`; `_kiso_sessions()` usa `COMP_WORDS` per rilevare `--instance/-i`; `explore)` in instance sub-commands chiama `_kiso_sessions`
- [x] `kiso/completions/kiso.zsh`: stesse modifiche; `_kiso_sessions()` usa `$words` (zsh completion global); `explore)` + `stats)` + `--session` top-level completati
- [x] `completions/kiso.bash` e `kiso.zsh`: sincronizzate

### 55c. REPL `/stats` slash command

- [x] `cli/__init__.py`: `/stats` in `_SLASH_COMMANDS`; `_slash_stats()` chiama `GET /admin/stats?since=7&session=...&by=model`; già implementato in M54

---

## Milestone 56: Audit doc e test — edge case M54-M55

**Obiettivo**: colmare gap di documentazione e test scoperti dopo M54-M55.

### 56a. Documentazione

- [x] `docs/api.md`: sezione completa `GET /admin/stats` (params, response JSON, 400/403)
- [x] `docs/audit.md`: sezione "Querying the Audit Log" con esempi `kiso stats` e tabella dimensioni
- [x] `docs/cli.md`: tabella error handling per `kiso stats`; sezione "Shell Completion" con esempi `source <(...)`
- [x] `docs/testing.md`: conteggi aggiornati (unit ~1670, bats 60)

### 56b. Test

- [x] `tests/test_stats.py`: `TestPrintStats` (6 test: righe vuote→"(no data)", tutti-unknown→colonna cost omessa, modello noto→colonna mostrata, singola riga, session filter nell'header, misto known/unknown); `TestFmtHelpers` (7 test: `_fmt_k` 0/999/1000/1.234M; `_fmt_cost` None/zero/piccolo/normale)
- [x] `tests/test_stats.py`: `test_unreadable_file_silently_skipped`, `test_missing_field_falls_back_to_unknown`
- [x] `tests/bash/test_host_stats.bats`: `--all` con 0 istanze mostra messaggio ed esce 0; `--instance` esplicito usa l'istanza specificata

---

## Milestone 57: install.sh — UX naming (bot name first, derive instance id)

**Problema**: il vecchio flusso chiedeva instance name e display name come domande consecutive, ordine contro-intuitivo e distinzione poco chiara per l'utente.

**Soluzione**: chiedere prima il nome display del bot, derivare automaticamente l'identifier via slug, mostrarlo per conferma/override.

- [x] `install.sh`: `_derive_instance_name()` — slugify: lowercase, spazi/underscore→trattini, strip non-alnum, collassa trattini multipli, trunca a 32, fallback "kiso"
- [x] `install.sh`: `ask_bot_and_instance_name()` — sostituisce `ask_instance_name()` + `ask_bot_name()`; globals `BOT_NAME` e `INST_NAME`; flusso: chiedi bot name [Kiso], mostra identifier derivato, chiedi conferma/override
- [x] `docs/docker.md`: sezione "Instance names" aggiornata con nota sul flusso di derivazione automatica
- [x] `tests/bash/test_install_derive_name.bats`: 10 test — passthrough, uppercase→lowercase, spazi→trattini, underscore→trattini, char speciali strippati, trattini consecutivi collassati, trattini leading/trailing strippati, truncate a 32, input vuoto→"kiso", cifre preservate

---

## Milestone 58: Rimozione codice morto — migration e legacy session scope

**Obiettivo**: eliminare codice di backward-compatibility per utenti/DB che non sono mai esistiti (software non ancora distribuito).

### 58a. install.sh — rimozione check layout mono-istanza

- [x] `install.sh`: rimosso blocco `OLD_CONFIG` (18 righe) che rilevava la vecchia struttura `~/.kiso/config.toml` e suggeriva la migrazione manuale a multi-istanza

### 58b. store.py — rimozione migration runtime e legacy session scope

- [x] `kiso/store.py`: rimossa `_migrate()` (54 righe) + FTS backfill — tutte le colonne già presenti nello SCHEMA base; nessun DB vecchio esiste
- [x] `kiso/store.py`: rimosso `OR session IS NULL` da `get_facts` e `search_facts` — residuo pre-M43 per fatti senza sessione; mai esistiti in produzione
- [x] `kiso/worker/loop.py`: reso esplicito `is_admin=True` nei due punti che chiamavano `get_facts(db)` senza argomenti (bump fact usage)
- [x] `tests/test_store.py`: rimosso `test_get_facts_null_session_user_fact_is_global`
- [x] `docs/flow.md`: rimosso punto "user facts with session IS NULL (legacy): treated as global"

---

## Milestone 59: Sicurezza — path traversal, pub token, install source injection

**Obiettivo**: correggere tre vulnerabilità individuate durante code review.

### 59a. Path traversal in `GET /pub/{token}/{path}` (`main.py:241`)

**Problema**: il check usa `str(file_path).startswith(str(pub_dir.resolve()))`. Se `pub_dir=/home/user/pub`, il path `/home/user/pub-evil/secret` supera il check perché è un prefisso stringa valido.

**Fix**: `file_path.resolve().is_relative_to(pub_dir.resolve())` (disponibile da Python 3.9).

- [ ] `kiso/main.py`: sostituire il check startswith con `Path.is_relative_to()`
- [ ] `tests/test_main.py`: aggiungere test `test_pub_path_traversal_rejected` — richiesta con `../` nel path viene rifiutata con 404

### 59b. Pub token fallback insicuro (`pub.py:15`)

**Problema**: `config.tokens.get("cli") or "kiso"` — se il token CLI non è configurato, la firma dei pub token usa la stringa indovinabile `"kiso"`.

**Fix**: sollevare `ConfigurationError` invece di usare il fallback.

- [ ] `kiso/pub.py`: rimuovere fallback `or "kiso"`; sollevare errore esplicito se token non configurato
- [ ] `tests/test_pub.py`: aggiungere test che verifica l'errore quando token mancante

### 59c. Code injection in `install.sh` via `source "$ENV_FILE"` (`install.sh:657`)

**Problema**: `set -a; source "$ENV_FILE"; set +a` — se `.env` contiene comandi shell (non solo assegnazioni `KEY=VALUE`), vengono eseguiti con i privilegi dello script di installazione.

**Fix**: parsare manualmente `KEY=VALUE` con lettura riga per riga, senza eseguire il file come script.

- [ ] `install.sh`: sostituire `source "$ENV_FILE"` con loop `while IFS='=' read -r key value` che esporta solo coppie con chiave `[A-Z_][A-Z0-9_]*`
- [ ] `tests/bash/test_install_register.bats` o nuovo file: test che un `.env` con un comando shell non lo esegue

---

## Milestone 60: Correttezza — atomicità store, transazioni, processi zombie

**Obiettivo**: correggere race condition, inconsistenza transazionale e resource leak individuati durante code review.

### 60a. `append_task_llm_call` non atomica (`store.py:340`)

**Problema**: legge JSON → deserializza → appende → riscrive. Se due coroutine scrivono sulla stessa task row in parallelo, una sovrascrive l'altra.

**Fix**: operazione atomica nativa SQLite con `json_insert`:
```sql
UPDATE tasks SET llm_calls = json_insert(COALESCE(llm_calls, '[]'), '$[#]', json(?)) WHERE id = ?
```

- [ ] `kiso/store.py`: riscrivere `append_task_llm_call()` con `json_insert` nativo SQLite
- [ ] `tests/test_store.py`: aggiungere test che verifica comportamento con chiamate concorrenti sullo stesso task

### 60b. `recover_stale_running` senza transazione (`store.py:510`)

**Problema**: due UPDATE separati (plans e tasks) senza transazione — se il processo muore tra i due, plans vengono resettati ma tasks restano `running`.

**Fix**: wrappare entrambi gli UPDATE in un'unica transazione esplicita.

- [ ] `kiso/store.py`: wrappare i due UPDATE in `recover_stale_running` in una singola transazione
- [ ] `tests/test_store.py`: aggiungere test che verifica che entrambe le tabelle vengano aggiornate atomicamente

### 60c. Processi zombie su `asyncio.TimeoutError` (`exec.py:35`, `skill.py:35`)

**Problema**: quando `asyncio.wait_for` scade, il processo figlio `asyncio.create_subprocess_exec` rimane in esecuzione. Nessun `proc.kill()` né `await proc.wait()` nel path di timeout.

**Fix**:
```python
except asyncio.TimeoutError:
    proc.kill()
    await proc.wait()
    raise
```

- [ ] `kiso/worker/exec.py`: aggiungere kill + wait nel handler `TimeoutError`
- [ ] `kiso/worker/skill.py`: stessa fix
- [ ] `tests/test_worker.py`: aggiungere test che verifica che su timeout il processo venga terminato

### 60d. Write non atomico del connector status (`connector.py:120`)

**Problema**: `_write_status()` scrive direttamente il file JSON — se un lettore accede a metà scrittura, ottiene JSON troncato/corrotto.

**Fix**: scrivere su `.tmp` e poi `os.replace()` (atomico su POSIX).

- [ ] `cli/connector.py`: riscrivere `_write_status()` con pattern write-tmp + replace
- [ ] `tests/test_cli_connector.py`: aggiungere test che verifica atomicità della scrittura

---

## Milestone 61: Performance — connection pooling, cache, transaction batching

**Obiettivo**: eliminare I/O e allocazioni ridondanti individuati durante code review.

### 61a. Connection pooling HTTP per chiamate LLM (`llm.py:90`)

**Problema**: nuovo `httpx.AsyncClient` ad ogni chiamata LLM — ogni volta nuovo handshake TLS. Con un piano da 10 task × 3 retry = 30+ connessioni TCP.

**Fix**: client condiviso inizializzato nel `lifespan()` di FastAPI e passato tramite `app.state`.

- [ ] `kiso/llm.py`: rimuovere `async with httpx.AsyncClient() as client:` inline; accettare client come parametro opzionale con fallback lazy
- [ ] `kiso/main.py`: inizializzare `httpx.AsyncClient` in `lifespan()`, passarlo a `call_llm` via contesto o parametro
- [ ] `tests/test_llm.py`: aggiornare mock per usare client iniettato

### 61b. Cache `discover_skills()` (`skills.py:50`)

**Problema**: `discover_skills()` legge tutti i `kiso.toml` dalla directory skills ad ogni pianificazione. La directory non cambia a runtime.

**Fix**: cache a livello di modulo con `_skills_cache: list[dict] | None = None` e funzione `invalidate_skills_cache()` chiamata dopo install/remove skill.

- [ ] `kiso/skills.py`: aggiungere cache + `invalidate_skills_cache()`
- [ ] `cli/skill.py`: chiamare `invalidate_skills_cache()` dopo install/remove/update
- [ ] `tests/test_skills.py`: aggiungere test che verifica che la cache venga usata e invalidata

### 61c. Audit dir init una sola volta (`audit.py:45`)

**Problema**: `mkdir(parents=True, exist_ok=True)` + `os.chmod(0o700)` eseguiti ad ogni scrittura nel log di audit.

**Fix**: flag `_audit_dir_ready: set[Path]` — la directory viene inizializzata solo al primo accesso.

- [ ] `kiso/audit.py`: estrarre `_ensure_audit_dir(path)` con guard `_audit_dir_ready`
- [ ] Test: verificare che `mkdir` venga chiamato una sola volta su N scritture

### 61d. `rglob` con `islice` per workspace grandi (`sysenv.py:180`)

**Problema**: `list(workspace.rglob("*"))[:max_files]` — se la workspace ha molti file, vengono tutti caricati in memoria prima del truncamento.

**Fix**: `list(itertools.islice(workspace.rglob("*"), max_files))`.

- [ ] `kiso/sysenv.py`: usare `islice` in `_collect_workspace_files()`

### 61e. `httpx.AsyncClient` singleton in `webhook.py` (`webhook.py:60`)

**Problema**: il client viene creato dentro il loop di retry — 3 tentative = 3 connessioni TCP.

**Fix**: creare il client fuori dal loop, usare `async with client:` che wrappa tutti i retry.

- [ ] `kiso/webhook.py`: spostare `httpx.AsyncClient` fuori dal loop di retry

### 61f. `rich.Console` singleton in `render.py` (`render.py:30`)

**Problema**: `Console()` istanziato ad ogni chiamata di rendering — inizializza terminale, rileva ANSI, alloca buffer.

**Fix**: `_console = Console()` e `_err_console = Console(stderr=True)` a livello di modulo.

- [ ] `cli/render.py`: rendere `Console` singleton di modulo

---

## Milestone 62: Refactor worker — dispatch, split `_process_message`, dedup task-step

**Obiettivo**: ridurre complessità ciclomatica e duplicazione nel cuore del worker.

### 62a. Dispatch pattern in `_execute_plan()` (`loop.py:400`)

**Problema**: if/elif da ~500 righe su tipo task (`exec`, `msg`, `skill`, `search`, `replan`). Aggiungere un nuovo tipo task richiede modificare questa funzione monolitica.

**Fix**: dict dispatch `_TASK_HANDLERS: dict[str, Callable] = {"exec": _handle_exec_task, ...}` — ogni handler è una funzione privata testabile in isolamento.

- [ ] `kiso/worker/loop.py`: estrarre `_handle_exec_task`, `_handle_msg_task`, `_handle_skill_task`, `_handle_search_task`, `_handle_replan_task`
- [ ] `kiso/worker/loop.py`: sostituire if/elif con `handler = _TASK_HANDLERS.get(task_type)` + chiamata
- [ ] `tests/test_worker.py`: aggiungere test per ogni handler in isolamento

### 62b. Deduplicare il pattern task-step (`loop.py:440`)

**Problema**: il ciclo "substatus → LLM → `_append_calls()` → `update_task_usage()` → `audit.log_task()` → handle failure" è copiato 5 volte (~100 righe duplicate) con piccole variazioni.

**Fix**: `async def _run_task_step(db, config, task, coro, ...) -> Result` che gestisce il ciclo comune; gli handler specifici passano solo la coroutine LLM.

- [ ] `kiso/worker/loop.py`: estrarre `_run_task_step()`
- [ ] Adattare i 5 handler a usarlo

### 62c. Split `_process_message()` (`loop.py:600`)

**Problema**: ~400 righe con 5+ livelli di nesting — gestisce setup sessione, fast-path, pianificazione, loop replan, post-processing, webhook, audit.

**Fix**: estrarre `_run_planning_loop()`, `_post_process_plan()`, `_finalize_message()`.

- [ ] `kiso/worker/loop.py`: estrarre le tre funzioni
- [ ] `tests/test_worker.py`: aggiungere test per `_run_planning_loop` e `_post_process_plan` in isolamento

---

## Milestone 63: Refactor plugin system — dedup install, validate, subprocess

**Obiettivo**: eliminare ~300+ righe di duplicazione tra skill e connector.

### 63a. Unificare `_skill_install` e `_connector_install` (`skill.py`, `connector.py`)

**Problema**: le due funzioni sono ~150 righe quasi identiche — clone → marker → validate → unofficial warning → `uv sync` → `deps.sh` → `check_deps` → `check_env_vars` → remove marker.

**Fix**: estrarre `_plugin_install(plugin_dir, plugin_type, validate_fn, check_deps_fn)` in `plugin_ops.py`.

- [ ] `cli/plugin_ops.py`: aggiungere `_plugin_install()` parametrizzato
- [ ] `cli/skill.py`: delegare a `_plugin_install`
- [ ] `cli/connector.py`: delegare a `_plugin_install`
- [ ] Test: verificare che il comportamento rimanga identico per entrambi i tipi

### 63b. Unificare `_validate_*_manifest` (`skills.py`, `connectors.py`)

**Problema**: `_validate_connector_manifest()` e `_validate_skill_manifest()` duplicano la stessa logica (campi obbligatori, formato versione, env vars, commands). Differiscono solo nei nomi dei campi.

**Fix**: `_validate_manifest(manifest, required_fields, manifest_type)` in un modulo condiviso.

- [ ] Estrarre in `cli/plugin_ops.py` o `kiso/plugins.py`
- [ ] Aggiornare i chiamanti

### 63c. Estrarre `_run_subprocess` da `exec.py` e `skill.py`

**Problema**: `run_exec()` e `run_skill()` hanno struttura identica — costruzione comando, `asyncio.create_subprocess_exec`, `wait_for`, `communicate`, gestione output/errori.

**Fix**: `async def _run_subprocess(cmd, env, timeout, cwd) -> SubprocessResult` in `worker/utils.py`.

- [ ] `kiso/worker/utils.py`: aggiungere `_run_subprocess()`
- [ ] `kiso/worker/exec.py`: delegare a `_run_subprocess`
- [ ] `kiso/worker/skill.py`: delegare a `_run_subprocess`

---

## Milestone 64: Refactor CLI e brain — split `_poll_status`, semplificare `_retry_llm`

**Obiettivo**: ridurre le funzioni più lunghe della CLI e del brain.

### 64a. Split `_poll_status()` (`cli/__init__.py:200`)

**Problema**: ~380 righe che mescolano polling, rendering, gestione spinner e condizioni di stop.

**Fix**: estrarre:
- `_render_plan_status(data, verbose) -> None` — solo rendering
- `_should_stop_polling(data) -> bool` — condizioni di uscita
- `_poll_status(session, ...)` — solo loop che chiama le altre

- [ ] `cli/__init__.py`: estrarre le tre funzioni
- [ ] `tests/test_cli.py`: aggiungere test per `_render_plan_status` e `_should_stop_polling` in isolamento

### 64b. Semplificare `_retry_llm_with_validation()` (`brain.py:65`)

**Problema**: gestisce 3 casi distinti (planner, reviewer, curator) con parametri tutti opzionali. Alta complessità ciclomatica.

**Fix**: separare in `_retry_with_json_schema()` e `_retry_with_key_check()`, oppure usare un callable `validator` iniettabile.

- [ ] `kiso/brain.py`: refactor in due helper separati
- [ ] `tests/test_brain.py`: aggiornare test esistenti

### 64c. Deduplicare setup in `_chat()` e `_msg_cmd()` (`cli/__init__.py:750`)

**Problema**: ~60 righe di setup identiche (config loading, token, user/session, httpx client) in due funzioni.

**Fix**: estrarre `_setup_client_context(args) -> ClientContext` (namedtuple o dataclass).

- [ ] `cli/__init__.py`: estrarre `_setup_client_context()`

---

## Milestone 65: Tech debt — TypedDict, cache prompts, bash robustness, log rotation

**Obiettivo**: risolvere il tech debt a bassa priorità accumulato durante lo sviluppo.

### 65a. TypedDict per entità store (`store.py`)

**Problema**: `dict[str, Any]` usato ovunque per session, message, task, fact — nessuna distinzione semantica, nessun supporto type checker.

**Fix**: definire `TypedDict` per le 4 entità principali: `Session`, `Message`, `Task`, `Fact`.

- [ ] `kiso/store.py`: aggiungere TypedDict; aggiornare signature delle funzioni store
- [ ] Aggiornare i chiamanti in `brain.py`, `loop.py`, `main.py`

### 65b. Cache `_load_system_prompt()` (`brain.py:120`)

**Problema**: i file di ruolo vengono riletti da disco ad ogni chiamata LLM. La cache `invalidate_cache()` già esiste per `sysenv` — estendere lo stesso pattern.

- [ ] `kiso/brain.py`: aggiungere `_prompt_cache: dict[str, str] = {}` e invalidazione in `invalidate_cache()`

### 65c. `MODEL_PRICES` — enforcement dell'ordine (`stats.py:20`)

**Problema**: il matching dipende dall'ordine delle chiavi nel dict — "more specific before less specific" è documentato ma non verificato. Aggiungere una chiave nel posto sbagliato rompe silenziosamente le stime.

**Fix**: aggiungere un test che verifica che `gemini-flash` sia matchato prima di `gemini` (se entrambi presenti); oppure passare a struttura a due livelli `{provider: {model: price}}`.

- [ ] `tests/test_stats.py`: aggiungere test di ordering per chiavi parziali

### 65d. `build_secret_variants()` silenzioso su eccezione (`security.py:120`)

**Problema**: se la costruzione delle varianti di un segreto fallisce (es. encoding error), l'eccezione viene ingoiata silenziosamente — il segreto viene incluso senza varianti e potrebbe leakare in output.

- [ ] `kiso/security.py`: aggiungere `logger.error(...)` nel blocco except; restituire almeno `{raw_secret}` come fallback esplicito

### 65e. Fix bash robustness: `set -euo pipefail` in `kiso-host.sh`, `base_url` unbound in `install.sh`

**Problema 1**: `kiso-host.sh` non usa `set -euo pipefail` — errori intermedi vengono ignorati.
**Problema 2**: `install.sh:639` — `base_url="${base_url:-...}"` può essere unbound con `set -u` se `NEED_CONFIG=false`.
**Problema 3**: `install.sh:300` — rilevamento non-interattivo usa `ARG_USER && ARG_API_KEY` invece di `[[ ! -t 0 ]]`.
**Problema 4**: `install.sh:450` — loop `while *--*` in `_derive_instance_name` è O(n²) su input degeneri. Fix: `raw=$(printf '%s' "$raw" | tr -s '-')`.

- [ ] `kiso-host.sh`: aggiungere `set -euo pipefail`; aggiungere `|| true` dove l'uscita in errore è indesiderata
- [ ] `install.sh`: inizializzare `base_url=""` in cima; usare `tr -s '-'` in `_derive_instance_name`; aggiungere check `[[ ! -t 0 ]]` in `ask_models`

### 65f. Log rotation per audit e session log (`audit.py`, `log.py`)

**Problema**: i file crescono indefinitamente — un file di audit per giorno (ok), ma i session log non vengono mai ruotati.

- [ ] `kiso/log.py`: usare `RotatingFileHandler` con `maxBytes` e `backupCount` configurabili
- [ ] `kiso/config.py`: aggiungere `log_max_bytes` e `log_backup_count` nei settings (default 5 MB, 3 backup)
- [ ] Aggiungere configurazione `audit_retain_days` e task di pulizia schedulato in `lifespan()`

### 65g. `SessionLogger` — aggiungere metodi `debug()` e `exception()` (`log.py:40`)

- [ ] `kiso/log.py`: aggiungere `debug()` ed `exception()` a `SessionLogger`

### 65h. Fixture `client` in `conftest.py` fragile rispetto a `lifespan` (`conftest.py:143`)

**Problema**: `app.state.config` e `app.state.db` impostati direttamente nella fixture. Se `lifespan()` inizializza nuovi campi, i test si rompono silenziosamente.

**Fix**: estrarre `_init_app_state(app, config, db)` da `lifespan()`, usata sia in produzione che nei test.

- [ ] `kiso/main.py`: estrarre `_init_app_state()`
- [ ] `tests/conftest.py`: aggiornare fixture `client` a usarla

---
